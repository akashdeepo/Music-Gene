{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from pydub import AudioSegment\n",
    "# from pydub.playback import play\n",
    "\n",
    "# # Function to generate a sine wave\n",
    "# def generate_sine_wave(frequency, duration, sample_rate=44100, amplitude=0.5):\n",
    "#     t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
    "#     wave = amplitude * np.sin(2 * np.pi * frequency * t)\n",
    "#     return wave\n",
    "\n",
    "# # Function to create an 8-bit music note\n",
    "# def create_note(frequency, duration, sample_rate=44100):\n",
    "#     wave = generate_sine_wave(frequency, duration, sample_rate)\n",
    "#     audio = np.int16(wave * 32767)  # Convert to 16-bit PCM\n",
    "#     return AudioSegment(audio.tobytes(), frame_rate=sample_rate, sample_width=2, channels=1)\n",
    "\n",
    "# # Define notes frequencies (C4, E4, G4, C5)\n",
    "# notes = {\n",
    "#     'C4': 261.63,\n",
    "#     'E4': 329.63,\n",
    "#     'G4': 392.00,\n",
    "#     'C5': 523.25,\n",
    "# }\n",
    "\n",
    "# # Create a simple melody\n",
    "# melody = [\n",
    "#     ('C4', 0.5),\n",
    "#     ('E4', 0.5),\n",
    "#     ('G4', 0.5),\n",
    "#     ('C5', 1.0),\n",
    "#     ('G4', 0.5),\n",
    "#     ('E4', 0.5),\n",
    "#     ('C4', 1.0),\n",
    "# ]\n",
    "\n",
    "# # Generate the music\n",
    "# music = AudioSegment.silent(duration=0)\n",
    "# for note, duration in melody:\n",
    "#     music += create_note(notes[note], duration * 0.5)\n",
    "\n",
    "# # Export the music to a .wav file\n",
    "# music.export(\"8bit_music.wav\", format=\"wav\")\n",
    "\n",
    "# # Play the music (optional)\n",
    "# play(music)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML-Dataset generationa and generating clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated a music-theory-based dataset with 1000 sequences, saved to theory_based_8bit_music_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Define notes frequencies for a C Major scale (C4, D4, E4, F4, G4, A4, B4, C5)\n",
    "notes = {\n",
    "    'C4': 261.63,\n",
    "    'D4': 293.66,\n",
    "    'E4': 329.63,\n",
    "    'F4': 349.23,\n",
    "    'G4': 392.00,\n",
    "    'A4': 440.00,\n",
    "    'B4': 493.88,\n",
    "    'C5': 523.25,\n",
    "}\n",
    "\n",
    "# Define note durations (quarter, half, whole notes, etc.)\n",
    "durations = [0.25, 0.5, 1.0, 2.0]  # In seconds\n",
    "\n",
    "# Define common chord progressions in the key of C major using individual notes\n",
    "chord_progressions = [\n",
    "    ['C4', 'F4', 'G4', 'C5'],  # I-IV-V-I\n",
    "    ['C4', 'G4', 'A4', 'F4'],  # I-V-vi-IV (using A4 instead of Am4)\n",
    "    ['C4', 'A4', 'F4', 'G4'],  # I-vi-IV-V (using A4 instead of Am4)\n",
    "    ['C4', 'F4', 'C5', 'G4'],  # I-IV-I-V\n",
    "]\n",
    "\n",
    "# Function to generate a random melody sequence with scale-based notes\n",
    "def generate_scale_based_melody(min_length, max_length):\n",
    "    length = random.randint(min_length, max_length)\n",
    "    melody = []\n",
    "    for _ in range(length):\n",
    "        note = random.choice(list(notes.values()))  # Ensure note is within the scale\n",
    "        duration = random.choice(durations)\n",
    "        melody.append((note, duration))\n",
    "    return melody\n",
    "\n",
    "# Function to generate a chord using a chord progression\n",
    "def generate_chord_from_progression():\n",
    "    progression = random.choice(chord_progressions)\n",
    "    chord_notes = [notes[note] for note in progression]\n",
    "    duration = random.choice(durations)\n",
    "    return (chord_notes, duration)\n",
    "\n",
    "# Function to generate a random sequence with melodies and chords\n",
    "def generate_theory_based_sequence(min_length, max_length):\n",
    "    sequence = []\n",
    "    for _ in range(random.randint(min_length, max_length)):\n",
    "        if random.random() < 0.7:  # 70% chance of a single note\n",
    "            sequence.append(generate_scale_based_melody(1, 1)[0])\n",
    "        else:  # 30% chance of a chord\n",
    "            sequence.append(generate_chord_from_progression())\n",
    "    return sequence\n",
    "\n",
    "# Generate an extensive dataset with music-theory-based sequences\n",
    "dataset = []\n",
    "num_sequences = 1000  # Number of sequences to generate\n",
    "min_sequence_length = 4  # Minimum length of each sequence\n",
    "max_sequence_length = 16  # Maximum length of each sequence\n",
    "\n",
    "for _ in range(num_sequences):\n",
    "    sequence = generate_theory_based_sequence(min_sequence_length, max_sequence_length)\n",
    "    dataset.append(sequence)\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "csv_file = 'theory_based_8bit_music_dataset.csv'\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write a header row with column names (optional)\n",
    "    header = ['Element_Type', 'Notes', 'Duration']\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Write each sequence element as a row in the CSV file\n",
    "    for sequence in dataset:\n",
    "        for element in sequence:\n",
    "            if isinstance(element[0], list):  # Chord\n",
    "                notes_str = \"-\".join([str(n) for n in element[0]])\n",
    "                writer.writerow(['Chord', notes_str, element[1]])\n",
    "            else:  # Single note\n",
    "                writer.writerow(['Note', element[0], element[1]])\n",
    "\n",
    "print(f\"Generated a music-theory-based dataset with {num_sequences} sequences, saved to {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.models import Model\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load the dataset\n",
    "csv_file = 'theory_based_8bit_music_dataset.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Function to parse notes and durations\n",
    "def parse_notes(note_str):\n",
    "    if '-' in note_str:\n",
    "        return [float(n) for n in note_str.split('-')]\n",
    "    else:\n",
    "        return float(note_str)\n",
    "\n",
    "data['Notes'] = data['Notes'].apply(parse_notes)\n",
    "data['Duration'] = data['Duration'].apply(float)\n",
    "\n",
    "# Normalize note frequencies\n",
    "notes_flat = []\n",
    "for note in data['Notes']:\n",
    "    if isinstance(note, list):\n",
    "        notes_flat.extend(note)\n",
    "    else:\n",
    "        notes_flat.append(note)\n",
    "\n",
    "notes_flat = np.array(notes_flat).reshape(-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "notes_flat_scaled = scaler.fit_transform(notes_flat)\n",
    "\n",
    "# Map back to original data\n",
    "note_idx = 0\n",
    "for i in range(len(data)):\n",
    "    if isinstance(data['Notes'].iloc[i], list):\n",
    "        note_count = len(data['Notes'].iloc[i])\n",
    "        data.at[i, 'Notes'] = notes_flat_scaled[note_idx:note_idx + note_count].flatten().tolist()\n",
    "        note_idx += note_count\n",
    "    else:\n",
    "        data.at[i, 'Notes'] = notes_flat_scaled[note_idx][0]\n",
    "        note_idx += 1\n",
    "\n",
    "# Prepare sequences for LSTM model\n",
    "sequence_length = 16  # Use a fixed length for LSTM input\n",
    "max_notes_per_step = max(len(note) if isinstance(note, list) else 1 for note in data['Notes'])\n",
    "\n",
    "X = []\n",
    "y_notes = []\n",
    "y_durations = []\n",
    "\n",
    "for i in range(len(data) - sequence_length):\n",
    "    notes_sequence = []\n",
    "    durations_sequence = []\n",
    "    for j in range(sequence_length):\n",
    "        note = data['Notes'].iloc[i + j]\n",
    "        if isinstance(note, list):\n",
    "            notes_sequence.extend(note + [0] * (max_notes_per_step - len(note)))  # Pad with zeros\n",
    "        else:\n",
    "            notes_sequence.extend([note] + [0] * (max_notes_per_step - 1))  # Pad with zeros for consistency\n",
    "        durations_sequence.append(data['Duration'].iloc[i + j])\n",
    "    \n",
    "    X.append(notes_sequence)\n",
    "    next_note = data['Notes'].iloc[i + sequence_length]\n",
    "    y_notes.append(next_note[0] if isinstance(next_note, list) else next_note)\n",
    "    y_durations.append(data['Duration'].iloc[i + sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y_notes = np.array(y_notes)\n",
    "y_durations = np.array(y_durations)\n",
    "\n",
    "# Reshape X for LSTM input (samples, time steps, features)\n",
    "n_features = max_notes_per_step  # Number of features per time step (notes per step)\n",
    "X = X.reshape((X.shape[0], sequence_length, n_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "158/158 [==============================] - 20s 102ms/step - loss: 0.6114 - notes_output_loss: 0.1354 - duration_output_loss: 0.4759\n",
      "Epoch 2/20\n",
      "158/158 [==============================] - 16s 100ms/step - loss: 0.5823 - notes_output_loss: 0.1324 - duration_output_loss: 0.4499\n",
      "Epoch 3/20\n",
      "158/158 [==============================] - 16s 100ms/step - loss: 0.5789 - notes_output_loss: 0.1306 - duration_output_loss: 0.4483\n",
      "Epoch 4/20\n",
      "158/158 [==============================] - 17s 105ms/step - loss: 0.5793 - notes_output_loss: 0.1307 - duration_output_loss: 0.4486\n",
      "Epoch 5/20\n",
      "158/158 [==============================] - 16s 102ms/step - loss: 0.5763 - notes_output_loss: 0.1301 - duration_output_loss: 0.4462\n",
      "Epoch 6/20\n",
      "158/158 [==============================] - 16s 103ms/step - loss: 0.5745 - notes_output_loss: 0.1300 - duration_output_loss: 0.4445\n",
      "Epoch 7/20\n",
      "158/158 [==============================] - 16s 101ms/step - loss: 0.5741 - notes_output_loss: 0.1300 - duration_output_loss: 0.4441\n",
      "Epoch 8/20\n",
      "158/158 [==============================] - 16s 102ms/step - loss: 0.5754 - notes_output_loss: 0.1305 - duration_output_loss: 0.4449\n",
      "Epoch 9/20\n",
      "158/158 [==============================] - 16s 101ms/step - loss: 0.5732 - notes_output_loss: 0.1298 - duration_output_loss: 0.4435\n",
      "Epoch 10/20\n",
      "158/158 [==============================] - 16s 100ms/step - loss: 0.5749 - notes_output_loss: 0.1299 - duration_output_loss: 0.4451\n",
      "Epoch 11/20\n",
      "158/158 [==============================] - 17s 107ms/step - loss: 0.5735 - notes_output_loss: 0.1300 - duration_output_loss: 0.4435\n",
      "Epoch 12/20\n",
      "158/158 [==============================] - 16s 103ms/step - loss: 0.5747 - notes_output_loss: 0.1297 - duration_output_loss: 0.4451\n",
      "Epoch 13/20\n",
      "158/158 [==============================] - 16s 102ms/step - loss: 0.5745 - notes_output_loss: 0.1293 - duration_output_loss: 0.4452\n",
      "Epoch 14/20\n",
      "158/158 [==============================] - 16s 101ms/step - loss: 0.5737 - notes_output_loss: 0.1296 - duration_output_loss: 0.4441\n",
      "Epoch 15/20\n",
      "158/158 [==============================] - 16s 102ms/step - loss: 0.5735 - notes_output_loss: 0.1294 - duration_output_loss: 0.4441\n",
      "Epoch 16/20\n",
      "158/158 [==============================] - 16s 100ms/step - loss: 0.5738 - notes_output_loss: 0.1293 - duration_output_loss: 0.4445\n",
      "Epoch 17/20\n",
      "158/158 [==============================] - 16s 103ms/step - loss: 0.5739 - notes_output_loss: 0.1298 - duration_output_loss: 0.4441\n",
      "Epoch 18/20\n",
      "158/158 [==============================] - 16s 102ms/step - loss: 0.5720 - notes_output_loss: 0.1294 - duration_output_loss: 0.4426\n",
      "Epoch 19/20\n",
      "158/158 [==============================] - 17s 105ms/step - loss: 0.5743 - notes_output_loss: 0.1296 - duration_output_loss: 0.4448\n",
      "Epoch 20/20\n",
      "158/158 [==============================] - 16s 100ms/step - loss: 0.5721 - notes_output_loss: 0.1291 - duration_output_loss: 0.4430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ac09d7d30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the LSTM model\n",
    "inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "x = LSTM(256, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "x = LSTM(256)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "output_notes = Dense(1, name='notes_output')(x)  # Predict the next note\n",
    "output_duration = Dense(1, name='duration_output')(x)  # Predict the next duration\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[output_notes, output_duration])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, [y_notes, y_durations], epochs=20, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a 20-second music sequence\n",
    "def generate_music(model, seed_sequence, num_notes, scaler):\n",
    "    generated_sequence = seed_sequence\n",
    "    generated_durations = []\n",
    "    \n",
    "    for _ in range(num_notes):\n",
    "        X_input = generated_sequence[-sequence_length:].reshape((1, sequence_length, -1))\n",
    "        prediction_notes, prediction_duration = model.predict(X_input, verbose=0)\n",
    "        \n",
    "        # Rescale prediction to original note frequencies\n",
    "        prediction_notes = scaler.inverse_transform(prediction_notes).flatten()\n",
    "        \n",
    "        # Ensure prediction_notes matches the number of features in generated_sequence\n",
    "        prediction_notes_padded = np.zeros((generated_sequence.shape[1],))\n",
    "        prediction_notes_padded[:len(prediction_notes)] = prediction_notes\n",
    "        \n",
    "        # Append predicted note and duration\n",
    "        generated_sequence = np.vstack([generated_sequence, prediction_notes_padded])\n",
    "        generated_durations.append(prediction_duration[0][0])\n",
    "    \n",
    "    return generated_sequence, generated_durations\n",
    "\n",
    "# Generate a new music sequence\n",
    "seed_sequence = X[0]  # Start with the first sequence in the dataset\n",
    "note_duration = 0.5  # Default duration in seconds if prediction fails\n",
    "total_duration = 20  # Total clip duration in seconds\n",
    "num_notes = int(total_duration / note_duration)\n",
    "\n",
    "generated_notes, generated_durations = generate_music(model, seed_sequence, num_notes, scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert seq to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akash\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='generated_8bit_music_20sec.wav'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment  # Import the necessary class\n",
    "\n",
    "# Function to create a sine wave for a note\n",
    "def generate_sine_wave(frequency, duration, sample_rate=44100, amplitude=0.5):\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
    "    wave = amplitude * np.sin(2 * np.pi * frequency * t)\n",
    "    return wave\n",
    "\n",
    "# Create the audio file from the generated sequence\n",
    "def create_note_sequence(notes, durations, sample_rate=44100):\n",
    "    music = AudioSegment.silent(duration=0)\n",
    "    for note, duration in zip(notes, durations):\n",
    "        wave = generate_sine_wave(note, duration, sample_rate)\n",
    "        audio = np.int16(wave * 32767)\n",
    "        note_segment = AudioSegment(audio.tobytes(), frame_rate=sample_rate, sample_width=2, channels=1)\n",
    "        music += note_segment\n",
    "    return music\n",
    "\n",
    "# Generate and save the audio\n",
    "generated_notes = generated_notes.flatten()\n",
    "music = create_note_sequence(generated_notes, generated_durations)\n",
    "music.export(\"generated_8bit_music_20sec.wav\", format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################## TEST 2 ##################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
